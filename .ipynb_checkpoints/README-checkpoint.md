# MMIFN: A Multi-Modal Interactive Fusion Network for Omnidirectional Image Quality Assessment

This repository contains the official implementation of **MMIFN**, a **multi-modal interactive fusion network** for **No-Reference Omnidirectional Image Quality Assessment (OIQA)**, as proposed in the paper:

> **"MMIFN: A Multi-Modal Interactive Fusion Network for Omnidirectional Image Quality Assessment"**  
> *Anonymous Submission, Journal of LaTeX Class Files*

This implementation:
- Introduces **textual semantic information** into OIQA via **DepictQA**.
- Fuses text with **global (VMamba)** and **local (ViT)** visual features.
- Achieves **SOTA performance** on **OIQA** and **CVIQ** datasets.
- Reproduces **Tables I–IV** from the paper.

---

## 📌 Key Features

- ✅ **First text-based OIQA model**
- ✅ **GFF Module**: Multi-scale global fusion via Bi-LSTM + Attention
- ✅ **LFF Module**: Fine-grained local fusion via LIT
- ✅ **AFM Module**: Attention-based aggregated fusion
- ✅ **Cross-dataset validation** and **ablation studies**
- ✅ **Modular, clean, and reproducible**

---

## 📁 Project Structure

```
MMIFN/
├── datasets/
│   ├── OIQA/
│   │   ├── images/                 # ERP-format omnidirectional images
│   │   ├── des.csv                 # image_name, MOS, distortion_type
│   │   └── viewports/              # 20 viewports per image
│   └── CVIQ/
│       ├── images/
│       ├── des.csv
│       └── viewports/
├── text/
│   └── des.csv                     # image_name, description (generated by DepictQA)
├── features/
│   ├── OIQA/
│   │   ├── text_features.pt
│   │   ├── viewport_features.pt
│   │   ├── global_features.pt
│   │   ├── global_fusion_features.pt
│   │   └── local_fusion_features.pt
│   └── CVIQ/                       # same structure
├── checkpoints/
│   └── mmifn_best.pth              # trained model weights
└── results/
    ├── predictions.pt
    ├── evaluation_results.txt      # Table I & II
    ├── cross_validation_results.txt # Table III
    └── ablation_results.txt        # Table IV
```

---

## 🛠️ Requirements

```bash
python >= 3.8
torch >= 2.0
torchvision
transformers
scipy
pandas
numpy
Pillow
scikit-learn
```

Install via:
```bash
pip install -r requirements.txt
```

or

```bash
bash env.sh
```

---

## 🔧 Setup Instructions

### 1. Prepare Datasets

Download and organize:
- **[OIQA Dataset](https://arxiv.org/abs/2207.02674)**
- **[CVIQ Dataset](https://ieeexplore.ieee.org/document/8702664)**

They can be found [here](https://github.com/sunwei925/MC360IQA)

Place in `datasets/OIQA/` and `datasets/CVIQ/`.

Ensure `des.csv` includes:
```csv
image_name,MOS,distortion_type
oiqa_img_001.png,46.287,JPEG
...
```

### 2. Generate Text Descriptions (Optional)

If the descriptions from DepictQA are not provided, generate text using `generate_text.py`:

```bash
python generate_text.py --dataset OIQA --prompt "Evaluate the image quality with a comprehensive explanation."
python generate_text.py --dataset CVIQ
```

Output: `text/des.csv`

### 3. Extract Features

Run in order:

```bash
# 1. Text features
python text_encoder.py

# 2. Viewport features
python viewport_encoder.py --dataset OIQA
python viewport_encoder.py --dataset CVIQ

# 3. Global visual features (VMamba)
python global_feature_extractor.py --dataset OIQA
python global_feature_extractor.py --dataset CVIQ

# 4. GFF: Global multimodal fusion
python gff_module.py --dataset OIQA
python gff_module.py --dataset CVIQ

# 5. LFF: Local multimodal fusion
python lff_module.py --dataset OIQA
python lff_module.py --dataset CVIQ
```

---

## 🚀 Training

Train on OIQA → test on OIQA and CVIQ:

```bash
python train.py
```

---

## 📊 Evaluation

### 1. In-Dataset Evaluation (Tables I & II)

```bash
python evaluate.py
```

Output: `results/evaluation_results.txt`

### 2. Cross-Dataset Validation (Table III)

```bash
python cross_eval.py
```

Output: `results/cross_validation_results.txt`

### 3. Ablation Study (Table IV)

```bash
python ablation_study.py
```

Output: `results/ablation_results.txt`

---

## 📈 Expected Results

### Table I: OIQA Performance

| Method       | Overall PLCC | SRCC  | RMSE  |
|-------------|-------------|-------|-------|
| **MMIFN (Ours)** | **0.986**     | **0.982** | **3.253** |

### Table II: CVIQ Performance

| Method       | Overall PLCC | SRCC  | RMSE  |
|-------------|-------------|-------|-------|
| **MMIFN (Ours)** | **0.984**     | **0.983** | **2.721** |

### Table III: Cross-Dataset Validation

| Train → Test | PLCC  | SRCC  | RMSE  |
|-------------|-------|-------|-------|
| CVIQ → OIQA | 0.864 | 0.847 | 4.866 |
| OIQA → CVIQ | 0.941 | 0.945 | 4.591 |

### Table IV: Ablation Study (OIQA PLCC)

| Model | PLCC |
|------|------|
| Full MMIFN (1) | **0.986** |
| w/o Text (3) | 0.942 |
| w/o GFF (4) | 0.658 |
| w/o LFF (5) | 0.881 |
| w/o AFM (6) | 0.923 |
| w/o LIT (7) | 0.951 |

---

## 🐳 Docker Setup (Optional)

Build the Docker image:

```Dockerfile
# Dockerfile
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

WORKDIR /mmifn

COPY . .

RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "train.py"]
```

Build and run:

```bash
docker build -t mmifn .
docker run --gpus all -v $(pwd)/datasets:/mmifn/datasets mmifn
```

---

## 📚 Citation

If you use this code or model, please cite the original paper. (It is not public and not even in arXiv. This is just a Lorem Ipsum bibtex):

```bibtex
@article{mmifn2024,
  title={MMIFN: A Multi-Modal Interactive Fusion Network for Omnidirectional Image Quality Assessment},
  author={Anonymous},
  journal={Journal of LaTeX Class Files},
  year={2024}
}
```

---

## 🙌 Acknowledgements

This work was supported by Raccoon Science and Technology Plan, Natural Science Foundation of Raccoon, and National Natural Science Foundation of Raccoon.

---

## 📬 Contact

For questions or feedback, please contact: `raccoon@example.com`
```

---

## ✅ `requirements.txt`

```txt
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
scipy>=1.10.0
pandas>=1.5.0
numpy>=1.21.0
Pillow>=9.0.0
scikit-learn>=1.2.0
matplotlib (optional)
seaborn (optional)
```

---

## ✅ Optional: `generate_text.py`

```python
# generate_text.py
from transformers import AutoProcessor, AutoModelForCausalLM
import pandas as pd
import os
from PIL import Image
import argparse

def generate_text_for_dataset(dataset_name, prompt):
    model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda")
    processor = AutoProcessor.from_pretrained("Qwen/Qwen-VL-Chat")

    image_dir = f"datasets/{dataset_name}/images"
    df = pd.read_csv(f"datasets/{dataset_name}/des.csv")
    descriptions = []

    for img_name in df['image_name']:
        img_path = os.path.join(image_dir, img_name)
        image = Image.open(img_path).convert("RGB")
        msgs = [{"role": "user", "content": [prompt, image]}]
        res = model.chat(processor, msgs)
        descriptions.append(res)

    df['description'] = descriptions
    df.to_csv(f"text/des_{dataset_name.lower()}.csv", index=False)
    print(f"Text descriptions saved for {dataset_name}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--prompt", type=str, default="Evaluate the image quality with a comprehensive explanation.")
    args = parser.parse_args()

    generate_text_for_dataset(args.dataset, args.prompt)
```

---

## ✅ Summary

You now have a **professional, shareable, and fully reproducible MMIFN project** that includes:

- ✅ `README.md` with full documentation
- ✅ `requirements.txt`
- ✅ `Dockerfile` for containerization
- ✅ `generate_text.py` for end-to-end pipeline
- ✅ Scripts to reproduce **Tables I–IV**


